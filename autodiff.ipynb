{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autodiff.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwJIjz/AeybwgWJ0ouWlsi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVCBcTq_amTZ"
      },
      "source": [
        "# Study of how to compute partial derivatives using pyTorch's autodiff\n",
        "This is needed in this context\n",
        " to compute the terms using for solving PDEs using Physics-Informed Neural Nets (PINNs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA89z_EkajpH"
      },
      "source": [
        "There are two main ways to obtain the (partial) derivatives/gradients of a pyTorch function (e.g., neural net):\n",
        " * __Using `autograd.grad`__: This function works well but expects a scalar input function. The sum is used because the derivative of a sum of variable with respect to one input variable depends only on the corresponding output variable.\n",
        " * __Using the tensor `backward` method__: This needs a starting tensor of ones with the same dimensions as the output so feed/backpropagate via the `backward` method. Gradients can be accessed on the `.grad` attribute.\n",
        "    * How to get second derivatives? <del datetime=\"2021-09-21\">_Major limitation?_</del>/><br/> Apply the `backward` method on the first derivative!\n",
        "    * Also requires more careful initialization and bookkeeping because we need to ensure `requires_grad=True` for the input tensors wrt to which one will need the derivatives. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NClHRw98ggSY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import autograd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L143SAiKVVIJ",
        "outputId": "8c4aad82-67e4-43f6-f481-037a7c6a8ded"
      },
      "source": [
        "# base model class to test approaches\n",
        "\n",
        "class TestNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._layers = [nn.Linear(2, 5),\n",
        "                        nn.Linear(5, 1)]\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        xx = torch.cat([x, t], axis=1)\n",
        "        h = torch.sigmoid(self._layers[0](xx))\n",
        "        y = self._layers[1](h)\n",
        "        return y\n",
        "\n",
        "\n",
        "xg, tg = np.meshgrid(np.linspace(-1, 1, 5), np.linspace(0, 1, 7))\n",
        "x = torch.as_tensor(xg.reshape((-1,1))).float().requires_grad_(True)\n",
        "t = torch.as_tensor(tg.reshape((-1,1))).float().requires_grad_(True)\n",
        "\n",
        "net = TestNet()\n",
        "u = net(x, t)\n",
        "print(u.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([35, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI9e5TzWkprz"
      },
      "source": [
        "### Using `autograd.grad` explicitly (on the sum)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmPUMqpce5ks",
        "outputId": "984dced8-b7bc-42a1-d996-33d81ac74786"
      },
      "source": [
        "# first-order derivatives\n",
        "ux = autograd.grad(u.sum(), x, create_graph=True)[0]  # du/dx\n",
        "ut = autograd.grad(u.sum(), t, create_graph=True)[0]  # du/dx\n",
        "print('du/dx =', ux.T)\n",
        "print('du/dt =', ut.T)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "du/dx = tensor([[-0.0807, -0.0830, -0.0814, -0.0763, -0.0689, -0.0810, -0.0829, -0.0808,\n",
            "         -0.0754, -0.0677, -0.0812, -0.0827, -0.0802, -0.0744, -0.0664, -0.0814,\n",
            "         -0.0824, -0.0795, -0.0733, -0.0652, -0.0815, -0.0820, -0.0787, -0.0722,\n",
            "         -0.0638, -0.0815, -0.0816, -0.0778, -0.0710, -0.0625, -0.0814, -0.0810,\n",
            "         -0.0768, -0.0697, -0.0611]], grad_fn=<PermuteBackward>)\n",
            "du/dt = tensor([[0.0068, 0.0064, 0.0073, 0.0095, 0.0126, 0.0057, 0.0056, 0.0067, 0.0091,\n",
            "         0.0124, 0.0046, 0.0047, 0.0061, 0.0088, 0.0123, 0.0036, 0.0039, 0.0056,\n",
            "         0.0084, 0.0121, 0.0026, 0.0031, 0.0050, 0.0081, 0.0119, 0.0016, 0.0023,\n",
            "         0.0045, 0.0077, 0.0117, 0.0006, 0.0016, 0.0040, 0.0074, 0.0115]],\n",
            "       grad_fn=<PermuteBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGVctIv0mTGe",
        "outputId": "84241ae2-f4a2-4038-973d-9ea1b4eac064"
      },
      "source": [
        "# second-order derivatives\n",
        "uxx = autograd.grad(ux.sum(), x, create_graph=True)[0]  # d^2u/dx^2\n",
        "print('d^2u/dx^2 =', uxx.T)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d^2u/dx^2 = tensor([[-0.0086, -0.0007,  0.0070,  0.0130,  0.0166, -0.0078,  0.0002,  0.0078,\n",
            "          0.0136,  0.0169, -0.0070,  0.0011,  0.0086,  0.0142,  0.0172, -0.0061,\n",
            "          0.0020,  0.0094,  0.0147,  0.0175, -0.0052,  0.0030,  0.0102,  0.0153,\n",
            "          0.0177, -0.0043,  0.0039,  0.0110,  0.0158,  0.0179, -0.0033,  0.0048,\n",
            "          0.0117,  0.0162,  0.0181]], grad_fn=<PermuteBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8k_EPlGlvPA"
      },
      "source": [
        "### Using tensor's `backward` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz0CE3mmb56Y",
        "outputId": "b11d43e7-bcca-41ee-b345-3b0bb96a1048"
      },
      "source": [
        "# first-order derivatives\n",
        "x.grad = None  # need to \"zero-out\" gradients on the vars of interest\n",
        "t.grad = None\n",
        "u.backward(gradient=torch.ones_like(u), create_graph=True)\n",
        "print('du/dx =', x.grad.T)\n",
        "print('du/dt =', t.grad.T)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "du/dx = tensor([[-0.0807, -0.0830, -0.0814, -0.0763, -0.0689, -0.0810, -0.0829, -0.0808,\n",
            "         -0.0754, -0.0677, -0.0812, -0.0827, -0.0802, -0.0744, -0.0664, -0.0814,\n",
            "         -0.0824, -0.0795, -0.0733, -0.0652, -0.0815, -0.0820, -0.0787, -0.0722,\n",
            "         -0.0638, -0.0815, -0.0816, -0.0778, -0.0710, -0.0625, -0.0814, -0.0810,\n",
            "         -0.0768, -0.0697, -0.0611]], grad_fn=<PermuteBackward>)\n",
            "du/dt = tensor([[0.0068, 0.0064, 0.0073, 0.0095, 0.0126, 0.0057, 0.0056, 0.0067, 0.0091,\n",
            "         0.0124, 0.0046, 0.0047, 0.0061, 0.0088, 0.0123, 0.0036, 0.0039, 0.0056,\n",
            "         0.0084, 0.0121, 0.0026, 0.0031, 0.0050, 0.0081, 0.0119, 0.0016, 0.0023,\n",
            "         0.0045, 0.0077, 0.0117, 0.0006, 0.0016, 0.0040, 0.0074, 0.0115]],\n",
            "       grad_fn=<PermuteBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUK8_IrQmP2M",
        "outputId": "8225ef8c-b99e-4059-fc6a-dcd2a9af6820"
      },
      "source": [
        "# second-order derivatives\n",
        "x.grad = None\n",
        "ux.backward(gradient=torch.ones_like(ux), create_graph=True)\n",
        "print('d^2u/dx^2 =', x.grad.T)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d^2u/dx^2 = tensor([[-0.0086, -0.0007,  0.0070,  0.0130,  0.0166, -0.0078,  0.0002,  0.0078,\n",
            "          0.0136,  0.0169, -0.0070,  0.0011,  0.0086,  0.0142,  0.0172, -0.0061,\n",
            "          0.0020,  0.0094,  0.0147,  0.0175, -0.0052,  0.0030,  0.0102,  0.0153,\n",
            "          0.0177, -0.0043,  0.0039,  0.0110,  0.0158,  0.0179, -0.0033,  0.0048,\n",
            "          0.0117,  0.0162,  0.0181]], grad_fn=<PermuteBackward>)\n"
          ]
        }
      ]
    }
  ]
}